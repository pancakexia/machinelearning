{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HFU87oU60rew",
        "outputId": "466c8f2c-35f3-46d5-fbf7-22ec7c4d44a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16512, 8)\n",
            "(16512, 1)\n",
            "(4128, 8)\n",
            "(4128, 1)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load dataset, split into training and test sets, and scale features\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "#from sklearn.datasets import load_boston\n",
        "california = datasets.fetch_california_housing(as_frame=False)\n",
        "#X, y = housing.data, housing.target\n",
        "# load boston housing price dataset\n",
        "\n",
        "x = california.data\n",
        "y = california.target\n",
        "\n",
        "# split into training and test sets, namely 80 percent of examples goes for the training, 20 percent goes for the test set\n",
        "N_train = int(0.8 * x.shape[0])\n",
        "x_train = x[:N_train,:]\n",
        "y_train = y[:N_train]\n",
        "x_test = x[N_train:,:]\n",
        "y_test = y[N_train:]\n",
        "\n",
        "x_train_mean = np.mean(x_train, axis=0)\n",
        "x_train_std = np.std(x_train, axis=0)\n",
        "x_test_mean = np.mean(x_test, axis=0)\n",
        "x_test_std = np.std(x_test, axis=0)\n",
        "# scale features by removing mean and dividing by the standard deviation\n",
        "x_train_scaled = (x_train-x_train_mean)/x_train_std# YOUR CODE GOES HERE\n",
        "x_test_scaled = (x_test-x_test_mean)/x_test_std# YOUR CODE GOES HERE\n",
        "#add one column in y_train and\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(x_train_scaled.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test_scaled.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XDiE7zUN0rex",
        "outputId": "cd736679-cab5-4ce0-b0da-19b442e3316b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16512, 9)\n",
            "(4128, 9)\n",
            "[[-0.8554256 ]\n",
            " [-0.32644825]\n",
            " [-0.07792875]\n",
            " [-0.19180322]\n",
            " [ 0.02556291]\n",
            " [ 1.32913022]\n",
            " [ 0.01677097]\n",
            " [ 2.04499342]\n",
            " [-1.12438438]]\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Add intercept terms and initialize parameters\n",
        "# Note: If you run this step again, please run from step 1 because notebook keeps the value from the previous run\n",
        "x_train_scaled = np.concatenate((np.ones((x_train_scaled.shape[0], 1)), x_train_scaled), axis=1)\n",
        "x_test_scaled = np.concatenate((np.ones((x_test_scaled.shape[0], 1)), x_test_scaled), axis=1)\n",
        "\n",
        "print(x_train_scaled.shape)\n",
        "print(x_test_scaled.shape)\n",
        "\n",
        "\n",
        "# init parameters using random values\n",
        "theta = np.random.randn(x_train_scaled.shape[1], 1)# YOUR CODE GOES HERE\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_Y-vqjuj0rey"
      },
      "outputs": [],
      "source": [
        "# Step 3: Implement the gradient and the cost function\n",
        "# In this step, you have to calculate the gradient. You can use the provided formula but the best way is to vectorize\n",
        "# that formula for efficiency\n",
        "def compute_gradient(x,y,theta):\n",
        "    # YOUR CODE GOES HERE\n",
        "    grad=(1/len(x)) * np.dot(x.T, (np.dot(x,theta)-y))\n",
        "    #grad=(1/len(x)) * ((x.T) @(x @ theta-y))\n",
        "    return grad\n",
        "def compute_cost(x,y,theta):\n",
        "    # YOUR CODE GOES HERE\n",
        "    cost = (1/(2*len(x)))* np.sum((np.dot(x,theta)-y)**2)\n",
        "    error = x @ theta - y\n",
        "    #cost = (1/(2*len(x)))* np.dot(error.T, error)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape[1]"
      ],
      "metadata": {
        "id": "KnuUndA0eBf7",
        "outputId": "dfefdf99-823f-41ad-d317-cc3f88dacbe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_qXyK7g80rey",
        "outputId": "74e785ef-b083-4e11-d4f2-3dea20286ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.967796943309741 10.968372162493294\n",
            "10.967943991183983 10.968225114619052\n",
            "10.968060918867812 10.968108186935222\n",
            "10.968058729795002 10.968110376008033\n",
            "10.96808516153732 10.968083944265715\n",
            "10.968188452925881 10.967980652877156\n",
            "10.968097638352754 10.968071467450283\n",
            "10.968399954498746 10.96776915130429\n",
            "10.96779459423918 10.968374511563855\n",
            "Gradient:  [[-2.87609592]\n",
            " [-1.40561718]\n",
            " [-0.23634034]\n",
            " [-0.25823107]\n",
            " [ 0.00608636]\n",
            " [ 1.03900024]\n",
            " [ 0.13085451]\n",
            " [ 3.15401597]\n",
            " [-2.89958662]]\n",
            "Approximate gradient:  [-2.87609592 -1.40561718 -0.23634034 -0.25823107  0.00608636  1.03900024\n",
            "  0.13085451  3.15401597 -2.89958662]\n",
            "Sum of gradient squared error:  6.545596520118234e-22\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Verify the gradient value\n",
        "# In this step, you need to verify that the computed gradient is correct. The difference betweet the gradient and the\n",
        "# approximate gradient should be very small (~10^-18)\n",
        "def approximate_gradient(x,y,theta,epsilon):\n",
        "    n_features = x.shape[1]\n",
        "    app_grad = np.zeros(n_features)\n",
        "    for i in range(n_features):\n",
        "        e = np.zeros(theta.shape)\n",
        "        e[i] = epsilon\n",
        "        cost_plus = compute_cost(x, y, theta + e)\n",
        "        cost_minus = compute_cost(x, y, theta - e)\n",
        "        print(cost_plus, cost_minus)\n",
        "        app_grad[i] = (cost_plus - cost_minus) / (2 * epsilon)# YOUR CODE GOES HERE\n",
        "    return app_grad\n",
        "\n",
        "grad = compute_gradient(x_train_scaled,y_train,theta)\n",
        "epsilon = 1e-4\n",
        "app_grad = approximate_gradient(x_train_scaled,y_train,theta,epsilon)\n",
        "print('Gradient: ',grad)\n",
        "print('Approximate gradient: ',app_grad)\n",
        "print('Sum of gradient squared error: ',np.sum((grad[:,0] - app_grad)**2))# add[:,0] to take out [] in grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LdRvFog60rey",
        "outputId": "4707e241-7154-4b9f-c713-cf1eb65a615a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-48-52add3a3b7fb>, line 32)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-52add3a3b7fb>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    J[step] = # YOUR CODE GOES HERE\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Try gradient descent algorithm with different learning rates\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# try different values for the learning rate\n",
        "learning_rates = [0.001,0.003,0.01,0.03,0.1,0.3]\n",
        "\n",
        "# this matrix keeps the learned parameters\n",
        "theta_matrix = np.zeros((len(learning_rates),x_train_scaled.shape[1]))\n",
        "\n",
        "# number of training iterations\n",
        "N_iterations = 100\n",
        "\n",
        "# prepare to plot\n",
        "plt.subplot(111)\n",
        "\n",
        "# calculate cost value and update theta\n",
        "for indx,alpha in enumerate(learning_rates):\n",
        "    # keep the cost value for each training step\n",
        "    J = np.zeros(N_iterations)\n",
        "\n",
        "    # initialize new parameters using random distribution\n",
        "    theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
        "    for step in range(N_iterations):\n",
        "        # update theta\n",
        "        theta = theta-learning_rates(step)*compute_gradient# YOUR CODE GOES HERE\n",
        "\n",
        "        # save the value of theta\n",
        "        theta_matrix[indx,:] = theta\n",
        "\n",
        "        # calculate the cost on traing set\n",
        "        J[step] = # YOUR CODE GOES HERE\n",
        "    # plot cost function\n",
        "    plt.plot(J)\n",
        "plt.xlabel('Training step')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend(('0.001','0.003','0.01','0.03','0.1','0.3'), loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hjqxXw60rey"
      },
      "outputs": [],
      "source": [
        "# Step 6: Predict the price of house\n",
        "# You have to select the best theta you found\n",
        "theta = # YOUR CODE GOES HERE\n",
        "predict_price = # YOUR CODE GOES HERE\n",
        "\n",
        "# calculate the cost for the test set\n",
        "test_cost = # YOUR CODE GOES HERE\n",
        "print('test cost: ',test_cost)\n",
        "\n",
        "# plot the ground truth and the predicted\n",
        "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
        "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
        "plt.legend(('Ground truth','Predicted'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Iu5V1LbK0rey"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}